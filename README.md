# AI Expense Tracker with RAG-Enhanced Financial Reasoning

Course: Database Systems (Final Project)  
Student: Raaj Patel   
Semester: Fall 2025

This project implements a **full-stack AI expense tracker** that combines:

- A **SQLite** relational database for transactions, categories, and budgets
- A **FastAPI** backend for CRUD operations, analytics, and a small RAG (Retrieval-Augmented Generation) layer
- A **Next.js frontend** (run with `pnpm`) for charts, tables, and chat-style interaction
- A **local LLM** served via **Ollama** to generate natural-language financial explanations grounded in SQL results

The goal is to build a **privacy-preserving financial copilot** that can answer questions such as:

- â€œWhy was my spending high last weekend?â€
- â€œWhich category pushed me over budget in December?â€
- â€œHow much did I spend on food this month compared to last month?â€

All numerical values in answers are **strictly fetched from SQLite**; the LLM is only used for explanation.

---

## 1. Project Structure

```text
Ai-Expese-Tracker/
â”œâ”€â”€ Backend/
â”‚   â””â”€â”€ backend/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ anomaly.py
â”‚       â”œâ”€â”€ cluster.py
â”‚       â”œâ”€â”€ db.py
â”‚       â”œâ”€â”€ load_csv_demo_data.py
â”‚       â”œâ”€â”€ main.py
â”‚       â”œâ”€â”€ models.py
â”‚       â”œâ”€â”€ rag.py
â”‚       â”œâ”€â”€ rulebased_rag.py
â”‚       â”œâ”€â”€ schemas.py
â”‚       â”œâ”€â”€ requirements.txt
â”‚       â”œâ”€â”€ .venv/                 # Python virtual environment
â”‚       â”œâ”€â”€ __pycache__/           # Python cache
â”‚       â”œâ”€â”€ data/
â”‚       â”‚   â”œâ”€â”€ budgets.csv
â”‚       â”‚   â”œâ”€â”€ categories.csv
â”‚       â”‚   â””â”€â”€ transactions.csv
â”‚       â”œâ”€â”€ db/
â”‚       â”‚   â”œâ”€â”€ analytics.sql
â”‚       â”‚   â”œâ”€â”€ indexes.sql
â”‚       â”‚   â””â”€â”€ schema.sql
â”‚       â”œâ”€â”€ docs/
â”‚       â”‚   â”œâ”€â”€ erd.md
â”‚       â”‚   â””â”€â”€ requirements.md
â”‚       â”œâ”€â”€ Image/
â”‚       â”‚   â”œâ”€â”€ ER_Diagram.png
â”‚       â”‚   â”œâ”€â”€ RDB_Text.png
â”‚       â”‚   â””â”€â”€ RDB.png
â”‚       â””â”€â”€ expense.db             # SQLite database file
â”‚
â””â”€â”€ Frontend/
    â”œâ”€â”€ app/
    â”‚   â”œâ”€â”€ anomalies/
    â”‚   â”‚   â””â”€â”€ page.tsx
    â”‚   â”œâ”€â”€ assistant/
    â”‚   â”œâ”€â”€ budgets/
    â”‚   â”œâ”€â”€ profile/
    â”‚   â”œâ”€â”€ transactions/
    â”‚   â”œâ”€â”€ globals.css
    â”‚   â”œâ”€â”€ layout.tsx
    â”‚   â””â”€â”€ page.tsx
    â”‚
    â”œâ”€â”€ components/
    â”‚   â””â”€â”€ ui/
    â”‚       â”œâ”€â”€ app-provider.tsx
    â”‚       â”œâ”€â”€ app-sidebar.tsx
    â”‚       â”œâ”€â”€ client-layout.tsx
    â”‚       â”œâ”€â”€ theme-provider.tsx
    â”‚       â””â”€â”€ top-bar.tsx
    â”‚
    â”œâ”€â”€ hooks/
    â”‚   â”œâ”€â”€ use-mobile.ts
    â”‚   â””â”€â”€ use-toast.ts
    â”‚
    â”œâ”€â”€ lib/
    â”‚   â”œâ”€â”€ api.ts
    â”‚   â””â”€â”€ utils.ts
    â”‚
    â”œâ”€â”€ public/
    â”‚
    â”œâ”€â”€ styles/
    â”‚   â””â”€â”€ globals.css
    â”‚
    â”œâ”€â”€ components.json
    â”œâ”€â”€ next-env.d.ts
    â”œâ”€â”€ next.config.mjs
    â”œâ”€â”€ package.json
    â”œâ”€â”€ pnpm-lock.yaml
    â”œâ”€â”€ postcss.config.mjs
    â”œâ”€â”€ tailwind.config.ts   (if present)
    â”œâ”€â”€ tsconfig.json
    â””â”€â”€ README.md   (optional frontend-only readme)

```

## 2. Key Features

- Natural-language queries over spending data
- RAG pipeline that converts questions â†’ SQL â†’ retrieved context â†’ grounded answer
- Local-only processing (SQLite + FastAPI + Ollama on your machine)
- Budget vs Actual comparisons per category
- Simple anomaly detection (high-spend days and categories)
- Interactive dashboard built with Next.js (run using pnpm)

### âš ï¸ Important: LLM / RAG Requirements

This project supports two modes of operation:

1. **LLM-powered RAG mode** (recommended)
2. **Rule-based fallback mode** (automatic backup)

To use the full RAG pipeline with natural-language explanations, you must install
and run **Ollama** locally:

- Download Ollama: https://ollama.com/download
- Pull at least one model: Example in these project test with ollama pull llama3.2

## ðŸ”„ Using a Different AI Model (Replacing Ollama)

The backend is designed so you can easily switch from **Ollama** to **any other LLM provider**, such as:

- OpenAI (GPT-4, GPT-4o, GPT-3.5)
- Groq (Llama-3, Mixtral Ultra-Fast)
- DeepSeek-R1 / DeepSeek-Chat
- HuggingFace Inference API
- Local models via LM Studio
- Custom inference servers (vLLM, TGI, llama.cpp, etc.)

Only **one file** must be modified:

### ðŸ”§ Modify This Function to Use Any AI Provider

The function responsible for LLM communication is:

```python
def call_ollama_chat(prompt: str, model: str = "llama3.2") -> str:
```

### Replace Ollama with OpenAI
```python
def call_ollama_chat(prompt: str, model: str = "gpt-4o"):
    from openai import OpenAI
    client = OpenAI(api_key="YOUR_API_KEY")

    response = client.chat.completions.create(
        model=model,
        messages=[{"role": "user", "content": prompt}],
    )
    return response.choices[0].message.content.strip()
```
### Replace with Groq (Llama-3 Turbo / Mixtral)
```python
def call_ollama_chat(prompt: str, model: str = "llama3-70b-8192"):
    import requests, os
    headers = {"Authorization": f"Bearer {os.getenv('GROQ_API_KEY')}"}

    resp = requests.post(
        "https://api.groq.com/openai/v1/chat/completions",
        headers=headers,
        json={
            "model": model,
            "messages": [{"role": "user", "content": prompt}]
        }
    )
    data = resp.json()
    return data["choices"][0]["message"]["content"].strip()
```

### Replace with HuggingFace Inference API
```python
def call_ollama_chat(prompt: str, model: str = "meta-llama/Meta-Llama-3-8B"):
    import requests, os
    HF_KEY = os.getenv("HF_TOKEN")

    response = requests.post(
        f"https://api-inference.huggingface.co/models/{model}",
        headers={"Authorization": f"Bearer {HF_KEY}"},
        json={"inputs": prompt},
    )
    text = response.json()[0]["generated_text"]
    return text.strip()
```
## 3. Prerequisites and Technologies Used

### Backend
- Python 3.11
- FastAPI
- Uvicorn
- SQLAlchemy
- Pydantic v2
- SQLite
- Ollama

### Frontend
- Node.js 18+
- Next.js
- React
- pnpm
- Recharts / D3.js

### for LLM / RAG Features

To enable natural-language explanations using a local LLM, install **Ollama** on your system.

**Install Ollama (Official Download):**  
https://ollama.com/download

After installation, pull at least one model (recommended: Llama 3.2):

```bash
ollama pull llama3.2
```

## 4. Backend Setup

All backend commands are run from the Backend/ directory.

### 4.1 Virtual Environment

```bash
cd ai-expense-tracker/Backend/backend
python3.11 -m venv .venv
source .venv/bin/activate
```

### 4.2 Install Dependencies

```bash
pip install -r requirements.txt
```

### 4.3 Run Backend

```bash
uvicorn main:app --reload
```

## 5. Frontend Setup

```bash
cd Frontend
pnpm install
```

Run frontend:

```bash
pnpm dev
```

## 6. Typical Workflow

```bash
ollama pull llama3.2
cd Backend
uvicorn main:app --reload
cd Frontend
pnpm dev
```

Visit: http://localhost:3000


